# IndoorCD: A Benchmark for 3D Point Cloud Change Detection in Indoor Environments

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Paper](https://img.shields.io/badge/Paper-IEEE%20Access-green.svg)](https://doi.org/10.1109/ACCESS.2025.XXXXXXX)

Official implementation of **"IndoorCD: A Benchmark Dataset and Methods for 3D Point Cloud Change Detection in Indoor Environments"** (IEEE Access 2025).

---

## ğŸ“‹ Overview

IndoorCD is a comprehensive benchmark dataset for evaluating 3D point cloud change detection methods in indoor environments. The dataset consists of **1,018 scene pairs** collected from **217 rooms** using iPhone LiDAR technology, featuring four types of changes: **Add**, **Remove**, **Move**, and **Composite**.

### Key Features

- ğŸ  **Real-world indoor scenes** captured with consumer-grade LiDAR
- ğŸ“¦ **3D bounding box annotations** for all changed objects
- ğŸ”„ **Multiple change types**: Add, Remove, Move, Composite
- ğŸ“Š **Comprehensive evaluation protocols**: Point-level, Box-level, Scene-level
- ğŸ§ª **9 baseline methods** including classical and deep learning approaches

---

## ğŸ“Š Benchmark Results

### Main Results (Point-Level Evaluation)

| Method | Type | Accuracy | Add F1 | Remove F1 | Macro F1 |
|--------|------|----------|--------|-----------|----------|
| **Multi-Stage (Ours)** | Classical | **95.7%** | **20.7%** | **28.8%** | **27.3%** |
| RANSAC-Based | Classical | 78.5% | 10.4% | 20.2% | 16.7% |
| ICP-Based | Classical | 71.5% | 9.0% | 19.3% | 15.4% |
| Distance-Based | Classical | 70.2% | 8.6% | 18.3% | 14.6% |
| DGCNN | Deep Learning | 86.2% | 3.1% | 9.4% | 16.7% |
| PointNet++ | Deep Learning | 85.7% | 4.0% | 8.7% | 16.5% |

### Key Findings

1. **Classical > Deep Learning**: Our geometric Multi-Stage method outperforms all learning-based approaches
2. **Remove detection is harder**: All methods show lower performance on Remove vs Add
3. **Object size matters**: Best detection accuracy (87.7%) for objects in 10-50L volume range

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/PasaCi/IndoorCD-Benchmark.git
cd IndoorCD-Benchmark

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or: venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### Download Dataset

Download the IndoorCD dataset from (link-coming-soon) (Available after paper acceptance)

```bash
# Expected directory structure
Dataset/
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ 001/
â”‚   â”‚   â”œâ”€â”€ 001-1.pcd    # Reference scan
â”‚   â”‚   â””â”€â”€ 001-2.pcd    # Comparison scan
â”‚   â”œâ”€â”€ 002/
â”‚   â””â”€â”€ ...
â””â”€â”€ Label/
    â”œâ”€â”€ 001-2.json       # Bounding box annotations
    â”œâ”€â”€ 002-2.json
    â””â”€â”€ ...
```

### Run Benchmark

```bash
# Run all methods on test set
python run_benchmark.py --data_path ./Dataset --output_dir ./results

# Run specific method
python run_benchmark.py --data_path ./Dataset --method multi_stage

# Run with custom parameters
python run_benchmark.py --data_path ./Dataset --method multi_stage \
    --iou_threshold 0.25 --seed 42

# Per-category evaluation
python run_benchmark.py --data_path ./Dataset --per_category
```

---

## ğŸ“ Repository Structure

```
IndoorCD-Benchmark/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ LICENSE                   # MIT License
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ setup.py                  # Package installation
â”œâ”€â”€ run_benchmark.py          # Main benchmark script
â”œâ”€â”€ config.yaml               # Default configuration
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ methods/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ classical.py      # All detection methods
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ metrics.py        # Evaluation metrics
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ data_loader.py    # Dataset loading utilities
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ default.yaml          # Default parameters
â”‚   â””â”€â”€ optimized.yaml        # Optimized parameters
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_dataset.py   # Dataset download helper
â”‚   â”œâ”€â”€ visualize_results.py  # Visualization tools
â”‚   â””â”€â”€ export_results.py     # Export to various formats
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ quick_start.py        # Basic usage example
â”‚   â”œâ”€â”€ custom_method.py      # Adding new methods
â”‚   â””â”€â”€ jupyter_demo.ipynb    # Interactive demo
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ DATASET.md            # Dataset documentation
    â”œâ”€â”€ METHODS.md            # Method descriptions
    â””â”€â”€ EVALUATION.md         # Evaluation protocols
```

---

## ğŸ”§ Methods

### 1. Multi-Stage (Proposed)

Our proposed method uses a multi-stage geometric approach:

1. **Coarse Detection**: KD-tree based point-to-point distance filtering
2. **Refinement**: Boundary analysis and proximity filtering
3. **Clustering**: DBSCAN-based spatial clustering
4. **Box Fitting**: Oriented bounding box generation with budget constraints

```python
from src.methods.classical import get_method

detector = get_method('multi_stage',
    distance_threshold=0.06,
    proximity_threshold=0.03,
    boundary_threshold=0.10,
    eps=0.015,
    min_samples=5,
    box_budget_per_class=5
)

boxes = detector.detect(reference_points, comparison_points)
```

### 2. Baseline Methods

| Method | Description |
|--------|-------------|
| `distance` | Nearest neighbor distance thresholding |
| `octree` | Voxel occupancy comparison |
| `icp` | ICP alignment + residual analysis |
| `ransac` | RANSAC plane removal + clustering |
| `region_growing` | Region-based segmentation |
| `m3c2` | Multi-scale model-to-model comparison |

---

## ğŸ“ˆ Evaluation Protocols

### Point-Level Evaluation

Evaluates per-point classification accuracy:
- **NoChange**: Points present in both scans
- **Add**: Points only in comparison scan
- **Remove**: Points only in reference scan

### Box-Level Evaluation (IoU = 0.25)

Matches predicted and ground truth bounding boxes:

```
IoU(pred, gt) = Volume(pred âˆ© gt) / Volume(pred âˆª gt)
```

### Scene-Level Classification

Binary classification: Does the scene contain any changes?

---

## ğŸ“ Configuration

### Default Parameters

```yaml
# config.yaml
dataset:
  train_ratio: 0.70
  val_ratio: 0.15
  test_ratio: 0.15
  seed: 42

evaluation:
  iou_threshold: 0.25
  
multi_stage:
  distance_threshold: 0.06
  proximity_threshold: 0.03
  boundary_threshold: 0.10
  eps: 0.015
  min_samples: 5
  tolerance_factor: 1.05
  roi_scale: 0.96
  roi_coverage_thresh: 0.5
  box_budget_per_class: 5
```

---

## ğŸ“š Citation

If you use this dataset or code in your research, please cite:

```bibtex
@article{author2025indoorcd,
  title={IndoorCD: A Benchmark Dataset and Methods for 3D Point Cloud Change Detection in Indoor Environments},
  author={Ciceklidag, Pasa and others},
  journal={IEEE Access},
  year={2025},
  volume={XX},
  pages={XXXXX-XXXXX},
  doi={10.1109/ACCESS.2025.XXXXXXX}
}
```


---

## ğŸ™ Acknowledgments

- Dataset collected using iPhone LiDAR (3D Scanner App)
- Point cloud processing: [Open3D](http://www.open3d.org/)
- Deep learning baselines adapted from [pytorch_geometric](https://pytorch-geometric.readthedocs.io/)

---

## ğŸ“§ Contact

For questions or issues, please open a GitHub issue or contact:
- **Email**: pasa.ciceklidag@research.uwa.edu.au pasaciceklidag@gmail.com
- **Project Page**:???

---

## ğŸ”„ Updates

